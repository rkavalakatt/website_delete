---
title: A1
author: ''
date: '2022-03-21'
slug: a1
categories: []
tags: []
---
###Questions
## a) The advantages of k-fold cross validation over the single split validation set approach is that single split validation splits the data into two groups, so if there is a lot of variation between the two groups that will carry into the validation and increase potential estimation error. Thus, k fold cross validation is better, also because single split validation only takes a part of the data to fit the model. Compared to LOOCV, however, the k fold cross validation does not perform as well. This is because LOOCV provides the most accurate estimate by using all the data. As such, LOOCV has less bias, but is subject to greater skew from outliers. 

## b) Bootstrapping is a simple and direct way to estimate statistics by sampling the dataset with replacement, and works with small or large data sets. Bootstrapping is disadvantageous, however, because it cannot make any assumptions about the shape of the data, and can take a lot of time and effort to perform. 

## c) Data pulled from Github below

## d) I used variables x3 (distance to the nearest MRT station) and x4 (number of convenience stores). I did not use variables like latitute or longitude, because they are not very useful without additional contextual information, and some of the other variables such as x4 did not have a very high correlation with the house price. I looked at different K values as well as different polynomial values for two variables, as well as different number of samples for hte bootstraping validation. 

```{r} 
library(ISLR)
library(boot)
```
# Cross Validation
## set the seed
```{r}
set.seed(1)
real_estate_data <- read.csv(file = "https://raw.githubusercontent.com/rkavalakatt/website_delete/main/content/post/2022-03-18-a1-modeling-for-prediction/Real%20estate%20valuation%20data%20set.csv")
head(real_estate_data)
dim(real_estate_data)
attach(real_estate_data)
```
### Sample 50% records
```{r}
train <- sample(414, 207)
head(train)
```
```{r} 
lm.fit.3 <- lm(Y.house.price.of.unit.area~X3.distance.to.the.nearest.MRT.station, data = real_estate_data, subset = train)
lm.fit.3
```
### Calculate error of the residual
```{r}
mean((Y.house.price.of.unit.area - predict(lm.fit.3,real_estate_data))[-train]^2)
```
###Plot
```{r}
plot(Y.house.price.of.unit.area~X3.distance.to.the.nearest.MRT.station) 
```
### Quadratic function
```{r}
lm.fit.poly.3 <- lm(Y.house.price.of.unit.area~ poly(X3.distance.to.the.nearest.MRT.station,2), data = real_estate_data, subset = train)
lm.fit.poly.3
```
### Find the error of the quadratic function(the error decreases)
```{r}
mean((Y.house.price.of.unit.area - predict(lm.fit.poly.3,real_estate_data))[-train]^2)

```
## Using Variable 4
```{r}
lm.fit.4 <- lm(Y.house.price.of.unit.area~X4.number.of.convenience.stores, data = real_estate_data, subset = train)
lm.fit.4
```
### Calculate error of the residual
```{r}
mean((Y.house.price.of.unit.area - predict(lm.fit.4,real_estate_data))[-train]^2)
```
###Plot
```{r}
plot(Y.house.price.of.unit.area~X4.number.of.convenience.stores) 
```
### Quadratic function
```{r}
lm.fit.poly.4 <- lm(Y.house.price.of.unit.area~ poly(X4.number.of.convenience.stores,2), data = real_estate_data, subset = train)
lm.fit.poly.4
```
### Find the error of the quadratic function(the error decreases)
```{r}
mean((Y.house.price.of.unit.area - predict(lm.fit.poly.4,real_estate_data))[-train]^2)

```
```{r}
glm.fit <- glm(Y.house.price.of.unit.area~X3.distance.to.the.nearest.MRT.station, data = real_estate_data)
coef(glm.fit)
lm.fit <- lm(Y.house.price.of.unit.area~X3.distance.to.the.nearest.MRT.station, data = real_estate_data)
coef(lm.fit)
```
```{r}
cv.err <- cv.glm(real_estate_data, glm.fit)
cv.err$delta
```
```{r}
cv.error <- rep(0,5)
degree <- 1:5
for (d in degree){
  glm.fit <- glm(Y.house.price.of.unit.area~poly(X3.distance.to.the.nearest.MRT.station,d), data = real_estate_data)
  cv.error[d] <- cv.glm(real_estate_data, glm.fit)$delta[1]
}
cv.error
```
```{r}
plot(degree, cv.error, type = "b")
```

### K For Cross Validation
```{r}
K = 2
cv.error.7 <- rep(0,5)
degree <- 1:5
for (d in degree){
  glm.fit <- glm(Y.house.price.of.unit.area~poly(X3.distance.to.the.nearest.MRT.station,d), data = real_estate_data)
  cv.error.7[d] <- cv.glm(real_estate_data, glm.fit, K = K)$delta[1]
}
cv.error.7
```
```{r}
plot(degree, cv.error, type = "b")
lines(degree, cv.error.7, type = "b", col = "red")
```
### K For Cross Validation
```{r}
K = 7
cv.error.7 <- rep(0,5)
degree <- 1:5
for (d in degree){
  glm.fit <- glm(Y.house.price.of.unit.area~poly(X3.distance.to.the.nearest.MRT.station,d), data = real_estate_data)
  cv.error.7[d] <- cv.glm(real_estate_data, glm.fit, K = K)$delta[1]
}
cv.error.7
```
```{r}
plot(degree, cv.error, type = "b")
lines(degree, cv.error.7, type = "b", col = "red")
```
## Perform Bootstrap Validation
```{r}
boot.fn <- function(data, index) {
  return(coef(lm(Y.house.price.of.unit.area~X3.distance.to.the.nearest.MRT.station, data = data, subset = index)))
}
```
###Sample size 2000
```{r}
boot.fn(real_estate_data, sample(414, 414, replace = T))
boot.out <- boot(real_estate_data, boot.fn, 2000)
plot(boot.out)
```

### Sample size 100
```{r}
boot.fn(real_estate_data, 100)
boot.out <- boot(real_estate_data, boot.fn, 100)
plot(boot.out)
```
## Perform Bootstrap Validation
```{r}
boot.fn <- function(data, index) {
  return(coef(lm(Y.house.price.of.unit.area~X4.number.of.convenience.stores, data = data, subset = index)))
}
```
###Sample size 2000
```{r}
boot.fn(real_estate_data, sample(414, 414, replace = T))
boot.out <- boot(real_estate_data, boot.fn, 2000)
plot(boot.out)
```
###Sample size 500
```{r}
boot.fn(real_estate_data, sample(414, 414, replace = T))
boot.out <- boot(real_estate_data, boot.fn, 500)
plot(boot.out)
```









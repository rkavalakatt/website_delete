---
title: A04 Ethics Assignment
author: ''
date: '2022-04-17'
slug: a04-ethics-assignment
categories: []
tags: []
---



### 1) What is misleading about the appended visualization and how you might go about fixing it. Follow the guidelines to answer the question

### The graph is misleading because each variable that is plotted has a different scale. The problemw ith dual-axis graphs, such as this one, is that if the scale of each axis is different, it might not be immediately obvious to the viewer, and can make it hard for most people to intuitively make correct conclusions comparing the two variables plotted. 

```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
```
```{r}
mask_data <- tribble(
  ~Date, ~No.Mask, ~Mask,
  "2020-7-12", 9.7, 25.4,
  "2020-7-13", 9.1, 19.8,
  "2020-7-14", 9.4, 19.7,
  "2020-7-15", 9.7, 20.4,
  "2020-7-16", 9.6, 20.1,
  "2020-7-17", 9.6, 20.1,
  "2020-7-18", 9.0, 20.4,
  "2020-7-19", 8.7, 20.0,
  "2020-7-20", 8.9, 20.6,
  "2020-7-21", 9.0, 21.6,
  "2020-7-22", 8.7, 19.8,
  "2020-7-23", 9.9, 19.9,
  "2020-7-24", 9.9, 20.5,
  "2020-7-25", 10.1, 19.0, 
  "2020-7-26", 9.7, 19.6,
  "2020-7-27", 9.7, 17.0,
  "2020-7-28", 9.7, 16.4,
  "2020-7-29", 10.1, 16.7, 
  "2020-7-30", 9.8, 16.7,
  "2020-7-31", 10.0, 16.0,
  "2020-8-1", 9.2, 16.3,
  "2020-8-2", 9.1, 16.1,
  "2020-8-3", 9.5, 16.0
)
```
## Plot the data
```{r}
ggplot(mask_data, aes(x = Date, y = value, color = variable)) + 
    geom_point(aes(y = Mask, col = "Mask")) + 
    geom_point(aes(y = No.Mask, col = "No.Mask")) +
    labs(title = "Kansas Covid-19 7-Day Rolling Average of Daily Cases / 100K Population", 
       subtitle = "Masked Vs Nonmasked Countries",
         x="Date", y = "Avg Cases / 100k Pop") +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```
##What message is more clear in your visualization than it was in the original visualization?
```{r}
###In the original visualization, it misrepresented the rate of Covid-19 cases in masked counties to make it seem like it was lower than in masked counties. My visualization shows that the number of total number of average cases in counties without masks are lower overall, but the rate of decrease of average case count is faster in counties with masks.
```
##What useful information do these data and your visualization tell us about mask wearing and COVID? 
```{r}
### This tells us important information about the effectiveness of mask wearing in preventing the spread of COVID-19 and lowering the number of daily average cases. From the visualization, we can see that while the overall number of daily average cases was higher in masked counties, the average daily case count is decreasing over time in these counties, showing that masks are effective at slowing the spread of COVID-19. In counties that did not use masks, the daily average for Covid cases stayed roughly consistent over time. An important thing to note is that the original visualization wanted to hide the fact that in unmasked counties, daily average Covid case counts were significantly lower than in masked counties. This was probably because whoever created this visualization wanted to encourage people to wear masks, and not assume that unmasked counties were safer.
```
## Video Review
### Joy Boulamwini explains her quest to fight bias in algorithms. Algorithmic bias can lead to exclusionary experiences and discriminatory practices. For example, in facial recognition algorithms, white faces are more easily recognizable than faces of other races. In testing her projects, she found that it was harder to train her models using faces of colors, and easier to train models using white faces. This is because there is racial bias built into generic facial recognition softwares, because they do not use inclusion training sets. These algorithms then become the basis of projects and startup companies around the world, spreading algorithmic bias even further. Full spectrum inclusion is very important in order to capture a more complete picture of humanity. This becomes increasingly important as facial recognition software is being employed by police departments to catch criminals, resulting in the bias built into our software seeping into our justice system and contribute to people of color being wrongly convicted of crimes. As more and more of decision-making becomes automated in everything from government to college admissions, flawed coding can impact millions of lives. The incoding movement strives to focus on identifying bias, curating inclusive trianing sets, and thinking more about the social impact of our code. Who codes, how we code, and why we code matters. 

##Datacamp Certificate of Completion










